# Decision Trees

  - A decision tree splits the data into multiple sets. Then, each of these sets is further split into subsets to arrive at a decision.
  - if a test splits the data into more than two partitions, this is called a multiway decision tree
  
  - In regression problems, a decision tree splits the data into multiple subsets. 
  - The difference between decision tree classification and decision tree regression is that in regression, 
  each leaf represents a linear regression model, as opposed to a class label.
  
  - Decision trees are easy to interpret; you can always go back and identify the various factors leading to a decision
  - A decision tree requires you to perform tests on attributes in order to split the data into multiple partitions
  - In classification, each data point in a leaf has a class label associated with it
  - There are some cases where a linear regression model cannot be used to make predictions, such as when you want to divide the 
  data set into multiple subsets because each subset has an independent trend corresponding to it. 
  There, you use a decision tree model to make predictions because a tree regression model has the capability of splitting the data 
  into multiple sets and assigning a linear regression model to each set independently.
  
# Algorithms for Decision Tree Construction

  - Always try to generate partitions that result in homogeneous data points. 
  - For classification tasks, a data set is completely homogeneous if it contains only a single class label.
  
# Gini Index


  
    
